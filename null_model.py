# -*- coding: utf-8 -*-
"""null_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NF3IM1NiLjz_qf_1qKz5PbQi_MmNdarP
"""

#@title Activating the GPU
# Main menu->Runtime->Change Runtime Type

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# Commented out IPython magic to ensure Python compatibility.
#@title Importing the modules
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.api.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertConfig
from transformers import  BertForSequenceClassification, get_linear_schedule_with_warmup
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm, trange
from sklearn.metrics import f1_score
import pandas as pd
import io
import numpy as np
import matplotlib.pyplot as plt


# %matplotlib inline

#@title Specifying CUDA as the device for Torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()
torch.cuda.get_device_name(0)

#@title Loading the Dataset
#source of dataset : https://stanfordnlp.github.io/contract-nli/
train_df = pd.read_csv("train_data_augmentation.csv")
val_df = pd.read_csv("dev.csv")
test_df = pd.read_csv("test.csv")
train_df.shape

train_df.sample(10)

#@title  Labels Encoding
label_mapping = {
    "Entailment": 0,
    "NotMentioned": 1,
    "Contradiction": 2,
}
train_df["label"] = train_df["label"].map(label_mapping)
test_df["label"] = test_df["label"].map(label_mapping)
val_df["label"] = val_df["label"].map(label_mapping)

train_df.to_csv("train_coded.csv", index=False)
val_df.to_csv("dev_coded.csv", index=False)
test_df.to_csv("test_coded.csv", index=False)
train_df.sample(10)

#@title Activating the Tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base")
def create_dataset(df):
    labels = torch.tensor(df['label'].values)
    encoded = tokenizer(
        df['text'].tolist(),
        df['hypothesis'].tolist(),
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )
    dataset = TensorDataset(encoded['input_ids'], encoded['attention_mask'], labels)
    return dataset

#@title Converting all the data into torch tensors
train_dataset = create_dataset(train_df)
val_dataset = create_dataset(val_df)
test_dataset = create_dataset(test_df)

torch.save(train_dataset, "train_dataset.pt")
torch.save(val_dataset, "val_dataset.pt")
torch.save(test_dataset, "test_dataset.pt")

train_dataset = torch.load("train_dataset.pt", weights_only=False)
val_dataset = torch.load("val_dataset.pt", weights_only=False)
test_dataset = torch.load("test_dataset.pt", weights_only=False)

print("Datasets cargados exitosamente.")

batch_size = 16

train_dataloader = DataLoader(train_dataset, sampler=SequentialSampler(train_dataset), batch_size=batch_size)
val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)
test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)

model = AutoModelForSequenceClassification.from_pretrained("microsoft/deberta-v3-small", num_labels=3, hidden_dropout_prob=0.5)

model.cuda()

from sklearn.utils.class_weight import compute_class_weight

y_train = train_df['label'].values

class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)

class_weights

#@title Loss function
from torch.nn import CrossEntropyLoss


class_weights = torch.tensor([0.67903683, 0.85,  2.85017836]).to(device)

loss_fct = CrossEntropyLoss(weight=class_weights)

#@title Optimizer Grouped Parameters
param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'LayerNorm.weight']

# Don't apply weight decay to any parameters whose names include these tokens.
# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01.
# - For the `bias` parameters, the 'weight_decay_rate' is 0.0.
optimizer_grouped_parameters = [
    {
        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
        'weight_decay': 0.1
    },
    {
        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
        'weight_decay': 0.0
    }
]

# Configuración del optimizador AdamW
optimizer = AdamW(
    optimizer_grouped_parameters,
    lr=3e-5,
    eps=1e-07,  # estabilidad numérica,
    betas=(0.9, 0.999)
)

from torch.optim.lr_scheduler import ReduceLROnPlateau


epochs = 4
lr_scheduler_type = "linear"
lr_scheduler_warmup_ratio = 0.01
total_steps = len(train_dataloader) * epochs
warmup_steps = int(total_steps * lr_scheduler_warmup_ratio)

scheduler = ReduceLROnPlateau(
    optimizer,
    mode='max',  # Usar 'max' si estás maximizando (como el F1-score), 'min' para minimizar
    factor=0.1,
    patience=3,
    verbose=True
)

def flat_f1(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, pred_flat, average='weighted')

t = []
train_loss_set = []

for epoch in trange(epochs, desc="Epoch"):

    # entrenamiento
    model.train()
    tr_loss = 0
    nb_tr_examples, nb_tr_steps = 0, 0

    for step, batch in enumerate(train_dataloader):
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch

        optimizer.zero_grad()


        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        logits = outputs['logits']


        loss = loss_fct(logits.view(-1, model.config.num_labels), b_labels.view(-1))

        train_loss_set.append(loss.item())


        loss.backward()


        optimizer.step()

        tr_loss += loss.item()
        nb_tr_examples += b_input_ids.size(0)
        nb_tr_steps += 1

    print(f"Train loss: {tr_loss/nb_tr_steps}")

    # validación
    model.eval()
    eval_loss, eval_f1 = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0

    for batch in val_dataloader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch

        with torch.no_grad():
            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)['logits']

        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        tmp_eval_f1 = flat_f1(logits, label_ids)

        eval_f1 += tmp_eval_f1
        nb_eval_steps += 1

    eval_f1 /= nb_eval_steps
    print(f"Validation F1: {eval_f1}")

    scheduler.step(eval_f1)